{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0a6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using h5py to unpack the archived file which holds the information for our images and labels\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d016bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in base dir: [('Test Data', <HDF5 group \"/Test Data\" (2 members)>), ('Train Data', <HDF5 group \"/Train Data\" (2 members)>)]\n",
      "items in group 1: [('x_train', <HDF5 dataset \"x_train\": shape (62870, 64, 64), type \"|u1\">), ('y_train', <HDF5 dataset \"y_train\": shape (62870,), type \"<i8\">)]\n",
      "items in group 2: [('x_test', <HDF5 dataset \"x_test\": shape (28080, 64, 64), type \"|u1\">), ('y_test', <HDF5 dataset \"y_test\": shape (28080,), type \"<i8\">)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55000, 64, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's extract our training, test and validation data\n",
    "#method seen in [1] p21\n",
    "with h5py.File('../HDF5/hdf5_uTHCD_compressed.h5', 'r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print(f'Items in base dir: {base_items}')\n",
    "    G1 = hdf.get('Train Data')\n",
    "    G1_items = list(G1.items())\n",
    "    print(f'items in group 1: {G1_items}')\n",
    "    \n",
    "    G2 = hdf.get('Test Data')\n",
    "    G2_items = list(G2.items())\n",
    "    print(f'items in group 2: {G2_items}')\n",
    "    \n",
    "    x_train = np.array(G1.get('x_train'))\n",
    "    y_train = np.array(G1.get('y_train'))\n",
    "    \n",
    "    x_test = np.array(G2.get('x_test'))\n",
    "    y_test = np.array(G2.get('y_test'))\n",
    "    \n",
    "    x_val = x_train[-7870:,:,:]\n",
    "    y_val = y_train[-7870:]\n",
    "    \n",
    "    x_train = x_train[:-7870,:,:]\n",
    "    y_train = y_train[:-7870]\n",
    "    \n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cacd1e8",
   "metadata": {},
   "source": [
    "Now that we have the data, we actually need to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a0ca1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7870, 64, 64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "#let's try an SVC\n",
    "#It's important to note here we are using very basic parameters\n",
    "svc = svm.SVC(gamma=0.001,kernel='poly', C = 100)\n",
    "\n",
    "#reshaping the data to work for SVC while maintaining images. Likely poor optimization\n",
    "x_train_svc = x_train.reshape(55000,-1)\n",
    "x_test_svc = x_test.reshape(28080, -1)\n",
    "x_val_svc = x_val.reshape(7870,-1)\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f726084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, gamma=0.001, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, gamma=0.001, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, gamma=0.001, kernel='poly')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we're gonna train on only 1000 samples until we figure out complexity issues\n",
    "SAMPLE_SIZE = 2000\n",
    "svc.fit(x_train_svc[:SAMPLE_SIZE],y_train[:SAMPLE_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5013198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5395"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = svc.predict(x_test_svc[:SAMPLE_SIZE])\n",
    "(predictions, y_test[:SAMPLE_SIZE])\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for pred, expect in zip(predictions,y_test[:SAMPLE_SIZE]):\n",
    "    if pred == expect:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "\n",
    "\n",
    "#some analysis of our model's performance with the linear kernel\n",
    "#accuracy:\n",
    "correct/SAMPLE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7ff00ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's see how it does on the validation data\n",
    "predictions = svc.predict(x_val_svc[:SAMPLE_SIZE])\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for pred, expect in zip(predictions,y_val[:SAMPLE_SIZE]):\n",
    "    if pred == expect:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "\n",
    "\n",
    "correct/SAMPLE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ecdd4",
   "metadata": {},
   "source": [
    "Likely slightly more accurate due to the validation data being closer in size to SAMPLE_SIZE than testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "183c47ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:46:06.346846: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-26 13:46:06.346883: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-26 13:46:06.348844: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-26 13:46:06.519770: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-26 13:46:08.015578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11bc4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions to compute f1 score later\n",
    "#taken from [3]\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1320c90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 31, 31, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              6423552   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 156)               80028     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7037948 (26.85 MB)\n",
      "Trainable params: 7037948 (26.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:46:16.832995: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2023-11-26 13:46:16.851592: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2023-11-26 13:46:16.867894: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25690112 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "#let's examine a couple examples of the training data:\n",
    "\n",
    "\n",
    "#set up and build the initial model of 2D convolutional layers and MaxPooling\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,1)))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "\n",
    "#will now flatten conv layers and add 3 dense layers of size 1024 and 512\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(156)) #156 distinct char classes per [1] p.3\n",
    "model.summary() #to output a depiction of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a14aa94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:46:20.402832: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 225280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:46:21.228218: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25690112 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 246s 142ms/step - loss: 1.9051 - accuracy: 0.6583 - f1_m: 2.8469 - precision_m: 1.5776 - recall_m: 16.2830 - val_loss: 0.9924 - val_accuracy: 0.7407 - val_f1_m: 2.1684 - val_precision_m: 1.1858 - val_recall_m: 12.7637\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 246s 143ms/step - loss: 0.4381 - accuracy: 0.8778 - f1_m: 2.0992 - precision_m: 1.1650 - recall_m: 10.7391 - val_loss: 0.8033 - val_accuracy: 0.7941 - val_f1_m: 2.0596 - val_precision_m: 1.1565 - val_recall_m: 9.4992\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 246s 143ms/step - loss: 0.2895 - accuracy: 0.9151 - f1_m: 2.0193 - precision_m: 1.1359 - recall_m: 9.2005 - val_loss: 0.8078 - val_accuracy: 0.7984 - val_f1_m: 2.0038 - val_precision_m: 1.1473 - val_recall_m: 7.9856\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 240s 140ms/step - loss: 0.2077 - accuracy: 0.9389 - f1_m: 1.9363 - precision_m: 1.1209 - recall_m: 7.2101 - val_loss: 0.8718 - val_accuracy: 0.8071 - val_f1_m: 1.9123 - val_precision_m: 1.1323 - val_recall_m: 6.2131\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 243s 141ms/step - loss: 0.1661 - accuracy: 0.9508 - f1_m: 1.8571 - precision_m: 1.1112 - recall_m: 5.7126 - val_loss: 0.8924 - val_accuracy: 0.8115 - val_f1_m: 1.8730 - val_precision_m: 1.1296 - val_recall_m: 5.5414\n"
     ]
    }
   ],
   "source": [
    "#compile the model using SparseCategoricalCrossentropy (see [2]) \n",
    "#here we only use 5 epochs to train\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy',f1_m,precision_m,recall_m]) \n",
    "res = model.fit(x_train,y_train, epochs=5, validation_data=(x_test,y_test)) #fit the training and test data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77715208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 153s 88ms/step - loss: 0.1766 - accuracy: 0.9477 - f1_m: 1.3008 - precision_m: 1.0774 - recall_m: 1.6602 - val_loss: 0.4824 - val_accuracy: 0.8772 - val_f1_m: 1.3073 - val_precision_m: 1.0841 - val_recall_m: 1.6585\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 153s 89ms/step - loss: 0.1437 - accuracy: 0.9560 - f1_m: 1.2447 - precision_m: 1.0659 - recall_m: 1.5034 - val_loss: 0.5277 - val_accuracy: 0.8694 - val_f1_m: 1.2680 - val_precision_m: 1.0820 - val_recall_m: 1.5413\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 153s 89ms/step - loss: 0.1331 - accuracy: 0.9600 - f1_m: 1.2240 - precision_m: 1.0600 - recall_m: 1.4556 - val_loss: 0.5660 - val_accuracy: 0.8709 - val_f1_m: 1.2810 - val_precision_m: 1.0744 - val_recall_m: 1.5967\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 149s 87ms/step - loss: 0.1224 - accuracy: 0.9641 - f1_m: 1.2026 - precision_m: 1.0558 - recall_m: 1.4041 - val_loss: 0.4919 - val_accuracy: 0.8714 - val_f1_m: 1.2387 - val_precision_m: 1.0931 - val_recall_m: 1.4379\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 164s 95ms/step - loss: 0.1041 - accuracy: 0.9677 - f1_m: 1.1620 - precision_m: 1.0455 - recall_m: 1.3137 - val_loss: 0.5262 - val_accuracy: 0.8787 - val_f1_m: 1.1732 - val_precision_m: 1.0750 - val_recall_m: 1.2982\n"
     ]
    }
   ],
   "source": [
    "#compile the model using SparseCategoricalCrossentropy (see [2]) \n",
    "#here we only use 5 epochs to train\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy',f1_m,precision_m,recall_m]) \n",
    "res = model.fit(x_train,y_train, epochs=5, validation_data=(x_test,y_test)) #fit the training and test data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad946072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 15s 17ms/step - loss: 0.5262 - accuracy: 0.8787 - f1_m: 1.1732 - precision_m: 1.0750 - recall_m: 1.2982\n",
      "loss: 0.5261752605438232\n",
      "acc: 0.8787037134170532\n",
      "f1: 1.173245906829834\n",
      "precision: 1.0749523639678955\n",
      "recall: 1.2981841564178467\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1, test_precision, test_recall = model.evaluate(x_test, y_test)\n",
    "print(f'loss: {test_loss}\\nacc: {test_acc}\\nf1: {test_f1}\\nprecision: {test_precision}\\nrecall: {test_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbe63f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 5s 22ms/step - loss: 0.5510 - accuracy: 0.8804 - f1_m: 1.8421 - precision_m: 1.1184 - recall_m: 5.2708\n",
      "loss: 0.5510273575782776\n",
      "acc: 0.8804320096969604\n",
      "f1: 1.8421167135238647\n",
      "precision: 1.1184182167053223\n",
      "recall: 5.270786762237549\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_f1, val_precision, val_recall = model.evaluate(x_val,y_val)\n",
    "print(f'loss: {val_loss}\\nacc: {val_acc}\\nf1: {val_f1}\\nprecision: {val_precision}\\nrecall: {val_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f447c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 12, 12, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 6, 6, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              1180672   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 156)               40092     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1895708 (7.23 MB)\n",
      "Trainable params: 1895708 (7.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#let's try another NN to compare\n",
    "#\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,1)))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "#will now flatten conv layers and add 3 dense layers of size 1024 and 512 and 256\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(156)) #156 distinct char classes per [1] p.3\n",
    "model.summary() #to output a depiction of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17aae899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 153s 88ms/step - loss: 1.8685 - accuracy: 0.5865 - f1_m: 4.2125 - precision_m: 9866204.0000 - recall_m: 8.5596 - val_loss: 0.8672 - val_accuracy: 0.7517 - val_f1_m: 1.8320 - val_precision_m: 1.2025 - val_recall_m: 3.8818\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 150s 88ms/step - loss: 0.4697 - accuracy: 0.8640 - f1_m: 1.8060 - precision_m: 1.1827 - recall_m: 3.8647 - val_loss: 0.6206 - val_accuracy: 0.8209 - val_f1_m: 1.8185 - val_precision_m: 1.1830 - val_recall_m: 3.9675\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 152s 88ms/step - loss: 0.3082 - accuracy: 0.9077 - f1_m: 1.7668 - precision_m: 1.1538 - recall_m: 3.8096 - val_loss: 0.5631 - val_accuracy: 0.8390 - val_f1_m: 1.7670 - val_precision_m: 1.1538 - val_recall_m: 3.7994\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 151s 88ms/step - loss: 0.2366 - accuracy: 0.9269 - f1_m: 1.7359 - precision_m: 1.1351 - recall_m: 3.7229 - val_loss: 0.4896 - val_accuracy: 0.8593 - val_f1_m: 1.7500 - val_precision_m: 1.1588 - val_recall_m: 3.6025\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 154s 89ms/step - loss: 0.1959 - accuracy: 0.9405 - f1_m: 1.6714 - precision_m: 1.1197 - recall_m: 3.3274 - val_loss: 0.4986 - val_accuracy: 0.8694 - val_f1_m: 1.7161 - val_precision_m: 1.1378 - val_recall_m: 3.5194\n"
     ]
    }
   ],
   "source": [
    "#compile the model using SparseCategoricalCrossentropy (see [2]) \n",
    "#here we only use 4 epochs to train\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy',f1_m,precision_m,recall_m]) \n",
    "res = model.fit(x_train,y_train, epochs=5, validation_data=(x_test,y_test)) #fit the training and test data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87d6d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 16s 18ms/step - loss: 0.4986 - accuracy: 0.8694 - f1_m: 1.7161 - precision_m: 1.1378 - recall_m: 3.5194\n",
      "loss: 0.4985743463039398\n",
      "acc: 0.8694444298744202\n",
      "f1: 1.7160624265670776\n",
      "precision: 1.1377657651901245\n",
      "recall: 3.519388437271118\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1, test_precision, test_recall = model.evaluate(x_test, y_test)\n",
    "print(f'loss: {test_loss}\\nacc: {test_acc}\\nf1: {test_f1}\\nprecision: {test_precision}\\nrecall: {test_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55130534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 5s 19ms/step - loss: 0.3586 - accuracy: 0.9113 - f1_m: 1.6865 - precision_m: 1.1288 - recall_m: 3.3729\n",
      "loss: 0.35860446095466614\n",
      "acc: 0.911308765411377\n",
      "f1: 1.6865177154541016\n",
      "precision: 1.1287736892700195\n",
      "recall: 3.3728694915771484\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_f1, val_precision, val_recall = model.evaluate(x_val,y_val)\n",
    "print(f'loss: {val_loss}\\nacc: {val_acc}\\nf1: {val_f1}\\nprecision: {val_precision}\\nrecall: {val_recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac8af8",
   "metadata": {},
   "source": [
    "REFERENCES:<br>\n",
    "[1]: N. Shaffi and F. Hajamohideen, \"uTHCD: A New Benchmarking for Tamil Handwritten OCR,\" in IEEE Access, vol. 9, pp. 101469-101493, 2021, doi: 10.1109/ACCESS.2021.3096823.<br>\n",
    "[2]: https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy <br>\n",
    "[3]: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model <br>\n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d89bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
